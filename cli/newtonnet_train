#! /usr/bin/env python

import os
import argparse
import yaml

import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR

from newtonnet.models import NewtonNet
from newtonnet.train import Trainer, EnergyForceLoss
from newtonnet.data import parse_train_test
from newtonnet.layers.activations import get_activation_by_string
from newtonnet.layers.cutoff import get_cutoff_by_string
from newtonnet.layers.aggregation import get_aggregation_by_string
from newtonnet.train.optimizer import get_optimizer_by_string, get_scheduler_by_string
# from newtonnet.data import parse_ani_data
# from newtonnet.data import parse_methane_data
# from newtonnet.data import parse_t1x_data

# torch.autograd.set_detect_anomaly(True)
# torch.set_default_tensor_type(torch.DoubleTensor)

# argument parser description
parser = argparse.ArgumentParser(
    description=
    'This is a pacakge to train NewtonNet on a given data.'
)
parser.add_argument(
    '-c',
    "--config",
    type=str,
    required=True,
    help="The path to the Yaml configuration file.")

# define arguments
args = parser.parse_args()
config = args.config

# locate files
settings_path = config
settings = yaml.safe_load(open(settings_path, 'r'))
settings_path = os.path.abspath(settings_path)
script_path = os.path.abspath(__file__)
output_path = settings['general']['output']

# device
if type(settings['general']['device']) is list:
    device = [torch.device(item) for item in settings['general']['device']]
else:
    device = [torch.device(settings['general']['device'])]

# data
torch.manual_seed(settings['data'].get('random_states', 42))
train_gen, val_gen, test_gen, normalizer = parse_train_test(settings, device[0])
print('normalizer: ', normalizer)

# model
# activation function
activation = get_activation_by_string(settings['model']['activation'])
cutoff_network = get_cutoff_by_string(settings['model']['cutoff_network'], settings['data']['cutoff'])
aggregation = get_aggregation_by_string(settings['model']['aggregation'])

model = NewtonNet(
    n_basis=settings['model']['resolution'],
    n_features=settings['model']['n_features'],
    activation=activation,
    n_layers=settings['model']['n_interactions'],
    dropout=settings['training']['dropout'],
    max_z=settings['model']['max_z'],
    cutoff=settings['data']['cutoff'],  ## data cutoff
    cutoff_network=cutoff_network,
    normalizer=normalizer,
    train_normalizer=settings['model']['train_normalizer'],
    requires_dr=settings['model']['requires_dr'],
    device=device[0],
    create_graph=True,
    share_layers=settings['model']['shared_interactions'],
    return_hessian=settings['model']['return_hessian'],
    double_update_latent=settings['model']['double_update_latent'],
    layer_norm=settings['model']['layer_norm'],
    aggregration=aggregation,
    )

# laod pre-trained model
if settings['model']['pre_trained']:
    model_path = settings['model']['pre_trained']
    model.load_state_dict(torch.load(model_path)['model_state_dict'])

# optimizer
trainable_params = filter(lambda p: p.requires_grad, model.parameters())
optimizer = get_optimizer_by_string(
    settings['training'].get('optimizer', 'adam'),
    trainable_params,
    **settings['training'].get('optimizer_kwargs', {}),
    )
lr_scheduler = get_scheduler_by_string(
    settings['training'].get('lr_scheduler', 'plateau'),
    optimizer,
    **settings['training'].get('lr_scheduler_kwargs', {}),
    )

# loss
loss_fn = EnergyForceLoss(
    w_energy=settings['model']['w_energy'],
    w_force=settings['model']['w_force'],
    w_f_mag=settings['model']['w_f_mag'],
    w_f_dir=settings['model']['w_f_dir'],
    wf_decay=settings['model']['wf_decay'],
    )

# training
trainer = Trainer(
    model=model,
    loss_fn=loss_fn,
    optimizer=optimizer,
    requires_dr=settings['model']['requires_dr'],
    device=device,
    settings_path=settings_path,
    output_path=output_path,
    script_path=script_path,
    lr_scheduler=lr_scheduler,
    checkpoint_log=settings['checkpoint']['log'],
    checkpoint_val=settings['checkpoint']['val'],
    checkpoint_test=settings['checkpoint']['test'],
    checkpoint_model=settings['checkpoint']['model'],
    verbose=settings['checkpoint']['verbose'],
    hooks=settings['hooks'],
    mode=settings['data']['mode'],
    )

# trainer.print_layers()

# tr_steps=1; val_steps=0; irc_steps=0; test_steps=0

trainer.train(
    train_generator=train_gen,
    epochs=settings['training']['epochs'],
    # steps=tr_steps,
    val_generator=val_gen,
    # val_steps=val_steps,
    irc_generator=None,
    irc_steps=None,
    test_generator=test_gen,
    # test_steps=test_steps,
    clip_grad=settings['training']['clip_grad'],
    )

print('done!')
